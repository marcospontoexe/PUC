{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pontifícia Universidade Católica do Paraná\n",
    "## Disciplina: Técnicas de Machine Learning\n",
    "#### Conteúdo complementar da Semana 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, carregarei todas as bibliotecas necessárias para trabalharmos neste notebook. Geralmente não há problema algum em você acabar fazendo alguns imports ao longo do notebook, mas confesso que fica melhor se você organizar todos os seus imports dentro de uma única célula.\n",
    "\n",
    "Estarei fazendo alguns comentários (iniciados com `#`) ao lado de cada `import` para lhe explicar para que serve cada biblioteca. Note que você não é obrigado a fazer isto para cada biblioteca, mas faço isto com a intenção de lhe explicar. Note que o texto _após_ o comentário fica com uma formatação diferente: isto serve para que você, ao observar o código, já consiga visualmente diferenciar o que é código do que não seria um código em si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # biblioteca para o carregamento de datasets a partir de arquivos em Excel, CSV e outros formatos\n",
    "import numpy as np # biblioteca para manipulação de vetores e matrizes grandes além de outras manipulações de dados de larga escala\n",
    "import matplotlib.cm as mcm # biblioteca para mostrar gráficos (espeficamente uma parte para cores)\n",
    "import matplotlib.pyplot as plt # biblioteca para mostrar gráficos (espeficamente uma parte para criar gráficos)\n",
    "import seaborn as sns # outra biblioteca para mostrar gráficos (ela é especificamente boa para alguns tipos de gráficos, como mapas de calor)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # importando somente o StandardScaler do scikit-learn\n",
    "from sklearn.datasets import load_wine # importando somente a função para obtermos o dataset wine (que já vem incluso no scikit-learn)\n",
    "from sklearn.feature_selection import * # importando todas as funções específicas de seleção de atributos do scikit-learn\n",
    "from sklearn.decomposition import * # importando todas as funções específicas para a extração de atributos do scikit-learn\n",
    "from sklearn.cluster import * # importando todas as funções específicas para o agrupamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset do Campeonato Brasileiro (Série A) de 2020\n",
    "\n",
    "Este dataset serviu para mostrar a importância do escalonamento dos dados (isto é, deixá-los com uma escala numérica similar e comparável) dentro do conteúdo apresentado. Primeiramente, lemos os dados da tabela do Brasileirão os quais estão em um arquivo de texto. Você pode abrir o arquivo dentro do Bloco de Notas ou aplicação similar. O separador (isto é, o que divide os valores) é o <kbd>Tab</kbd> (tabulação).\n",
    "\n",
    "Viu lá em cima que usamos o `import pandas as pd` ao invés de `import pandas`? O `as pd` serve para que façamos chamadas de código mais enxutas. Logo, é mais fácil digitar `pd.read_csv` do que `pandas.read_csv`.\n",
    "\n",
    "Como o separador é o <kbd>Tab</kbd>, informamos que o separador (`sep`) é o <kbd>Tab</kbd>. O operador `\\t` é outra forma de nos referirmos à tecla <kbd>Tab</kbd>. Como sabemos disso? <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\">Consultando sempre a documentação do Pandas</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('brasileirao.tsv', sep='\\t') # lendo o arquivo. O resultado ficará no dataframe chamado \"df\"\n",
    "df.tail() # o \"tail\" mostra o final do dataset. Se não colocarmos nada entre os parênteses teremos por padrão as últimas 5 linhas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação para a normalização\n",
    "\n",
    "Aqui, mostrarei dois gráficos: um deles utiliza as colunas `Derrotas` e `Saldo de Gols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5)) #criando uma figura com o mesmo tamanho nos dois eixos (isto é, uma imagem quadrada)\n",
    "plt.scatter(df['Derrotas'], df['Saldo de Gols'], color=mcm.rainbow(np.linspace(0, 1, 20))) # mostrando um gráfico de dispersão (scatterplot) onde o eixo x representa os valores da coluna derrotas, o eixo y representa o saldo de gols e, finalmente, colocamos uma escala de cores para as bolinhas. As bolinhas são geradas de acordo com a ordem das linhas no dataframe\n",
    "plt.xlim((-35,25)) # colocamos uma escala numérica para o eixo x (opcional, mas aqui é importante para vermos as diferentes ordens de grandeza)\n",
    "plt.ylim((-35,25)) # colocamos uma escala numérica para o eixo y (opcional, mas aqui é importante para vermos as diferentes ordens de grandeza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, aplicarei o `StandardScaler` para padronizar as duas colunas. No final, mostro o dataset completo. Note que a escala numérica dessas duas colunas mudou. Depois, somente rodamos novamente o gráfico (agora com os valores já padronizados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Derrotas', 'Saldo de Gols']] = StandardScaler().fit_transform(df[['Derrotas', 'Saldo de Gols']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(df['Derrotas'], df['Saldo de Gols'], color=mcm.rainbow(np.linspace(0, 1, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset de vinhos\n",
    "\n",
    "Este dataset serviu para testarmos algumas técnicas de aprendizagem não-supervisionada. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine\">Ele é um toy dataset que já vem incluso no scikit-learn</a>. Note que possui dois atributos adicionais: `as_frame` (o qual estamos usando para que já esteja no formato compatível com o Pandas, o que ajuda muito na visualização) e `return_X_y` (o qual retorna dois dados somente: o dataframe com os dados de entrada - ou seja, todas as características dos vinhos; e os dados de saída - ou seja, a classe/label que estamos analisando. Aqui, seriam os três produtores de vinho. O `display` é uma função bem parecida com o `print` do Python, mas que permite uma melhor visualização dentro do Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine, target_wine = load_wine(as_frame=True, return_X_y=True)\n",
    "display(df_wine.tail())\n",
    "display(target_wine.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção de Atributos\n",
    "### VarianceThreshold\n",
    "Aqui testamos diferentes técnicas na mesma base de dados (o `df_wine`). Primeiro, testamos o <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold\">VarianceThreshold</a>. Como ele somente retorna uma matriz (lembra na disciplina de Raciocínio Computacional?), perdemos os nomes das colunas. Por outro lado, compare os resultados conforme vamos alterando o `threshold` (limite aceitável).\n",
    "\n",
    "Ah, e antes que me esqueça: o pd.DataFrame cria um novo dataframe (já que o VarianceThreshold retorna uma matriz, como comentei) e contendo somente as colunas que importam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for limite in [0.0, 0.1, 0.5, 0.8, 1.0]:\n",
    "    display(f'Testando com um threshold de {limite}. Resultado:')\n",
    "    sel = VarianceThreshold(threshold=limite)\n",
    "    display(pd.DataFrame(sel.fit_transform(df_wine), columns=df_wine.columns[sel.get_support()]).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembra do `StandardScaler` que vimos há pouco? Vamos ver como ficaria com exatamente a mesma tabela, mas após usá-lo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_scaled = StandardScaler().fit_transform(df_wine)\n",
    "\n",
    "for limite in [0.0, 0.1, 0.5, 0.8, 1.0]:\n",
    "    display(f'Testando com um threshold de {limite} (com StandardScaler). Resultado:')\n",
    "    sel = VarianceThreshold(threshold=limite)\n",
    "    display(pd.DataFrame(sel.fit_transform(df_wine_scaled), columns=df_wine.columns[sel.get_support()]).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest\n",
    "\n",
    "Como já criamos o `df_wine_scaled` anteriormente, não precisaremos recriá-lo aqui. Vamos comparar os resultados do `SelectKBest` para o `df_wine` antes e após a sua normalização. O `.shape` retorna a quantidade de **linhas e colunas** de um dataframe. Já o `.shape[0]` retorna só a quantidade de linhas (igual ao `len`, visto em Raciocínio Computacional) e o `shape[1]` retorna só a quantidade de colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for limite in range(df_wine.shape[1]):\n",
    "    display(f'Testando com um k={limite}. Resultado:')\n",
    "    sel = SelectKBest(k=limite)\n",
    "    display(pd.DataFrame(sel.fit_transform(df_wine, target_wine), columns=df_wine.columns[sel.get_support()]).tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for limite in range(df_wine.shape[1]):\n",
    "    display(f'Testando com um k={limite} (com StandardScaler). Resultado:')\n",
    "    sel = SelectKBest(k=limite)\n",
    "    display(pd.DataFrame(sel.fit_transform(df_wine_scaled, target_wine), columns=df_wine.columns[sel.get_support()]).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for limite in [5, 10, 50, 80, 100]:\n",
    "    display(f'Testando com um percentile={limite}. Resultado:')\n",
    "    sel = SelectPercentile(percentile=limite)\n",
    "    display(pd.DataFrame(sel.fit_transform(df_wine, target_wine), columns=df_wine.columns[sel.get_support()]).tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for limite in [5, 10, 50, 80, 100]:\n",
    "    display(f'Testando com um percentile={limite}. Resultado:')\n",
    "    sel = SelectPercentile(percentile=limite)\n",
    "    display(pd.DataFrame(sel.fit_transform(df_wine_scaled, target_wine), columns=df_wine.columns[sel.get_support()]).tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de atributos\n",
    "### PCA\n",
    "A documentação do scikit-learn possui uma <a href=\"https://scikit-learn.org/stable/modules/decomposition.html\">explicação bem interessante sobre PCA</a>, caso tenha interesse. Aqui, o PCA é basicamente sobre extrair _novos_ atributos a partir dos atributos já existentes. Sendo assim, vamos aplicar o PCA no exemplo do wine. Consulte a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\">documentação específica do PCA</a> para entender que tipo de operações seriam possíveis com ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=df_wine.shape[1]) # colocando que queremos que o número de componentes seja igual ao que já temos hoje para fins de visualização e estudo\n",
    "pca.fit(df_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5)) # criando um gráfico retangular para facilitar a visualização\n",
    "plt.plot(pca.explained_variance_ratio_, color='r') # colocando a porcentagem de variância que cada componente nos trouxe\n",
    "plt.xticks(np.arange(df_wine.shape[1])) # mostrando todos os números no eixo x\n",
    "plt.show() # mostrando o gráfico final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, como interpretamos isso? Existe um método chamado **elbow method** (ou _método do cotovelo_) que pode ajudar. Basicamente olhamos da esquerda para direita e paramos quando identificamos um \"cotovelo\" no gráfico: isto é, indo para a direita veríamos que não teríamos mais grandes alterações na curva indicando, assim, que haveríamos chegado à quantidade ideal de componentes e não valeria o esforço selecionarmos mais componentes. Olhando para o gráfico acima fica fácil: à direita do **1** no eixo x não vemos uma mudança no comportamento: logo, os componentes 0 e 1 seriam os mais adequados (em outras palavras, somente 2 componentes).\n",
    "\n",
    "Por outro lado, o componente 0 parece ser estranhamente relevante em relação aos demais, não é? Olhando a documentação do scikit-learn sobre o PCA encontramos um item interessante chamado `components_`. Com ele, conseguimos ver a contribuição de cada um dos atributos (abaixo representados pelas linhas) sobre os componentes (representados abaixo pelas colunas). Note que no componente **0** temos um valor bem destoante do `flavanoids` em relação aos demais. Isto indicaria que somente esta coluna impactaria muito para detectar o produtor de um vinho. Será que é _só isso mesmo_ que influencia? Pensemos em um caso real: será que para tomarmos decisões não acabamos olhando para um conjunto de atributos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(pd.DataFrame(pca.components_, index=df_wine.columns), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será que não faltaria normalizarmos os dados? Vejamos o mesmo PCA, mas agora com o `df_wine_scaled`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=df_wine_scaled.shape[1])\n",
    "pca.fit(df_wine_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5)) # criando um gráfico retangular para facilitar a visualização\n",
    "plt.plot(pca.explained_variance_ratio_, color='r') # colocando a porcentagem de variância que cada componente nos trouxe\n",
    "plt.xticks(np.arange(df_wine.shape[1])) # mostrando todos os números no eixo x\n",
    "plt.show() # mostrando o gráfico final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece mais justo, não é? Podemos ter um elbow já no **3** (ou seja, 4 componentes) ou, ainda, lá no **7** (ou seja, 8 componentes já que a contagem começou no 0 e não no 1). Em casos reais geralmente queremos menos componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(pd.DataFrame(pca.components_, index=df_wine.columns), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melhor! Veja que no primeiro componente temos uma contribuição de fatores liderados pelo `proanthocyanins` e `malic_acid`. Já no segundo componente, `total_phenols`, `flavanoids`, `alcalinity_of_ash` e, em menor escala, `color_intensity`. Já no terceiro componente, `ash` e `hue` (de forma antagônica - veja que um é positivo e outro com sinal negativo). No quarto, `ash`, `hue` e `nonflanavoid_phenols` (agora com `ash` e `hue` com o mesmo sinal). Veja que todas as colunas influenciam em maior ou menor grau nos componentes.\n",
    "\n",
    "De forma geral, é possível darmos um nome para os componentes de acordo com as colunas que mais trazem impacto e a relação entre elas: olhemos o componente **7**: os `flavanoids`, `magnesium` e `alcohol` trazem os maiores impactos. Os `flavanoids` são responsáveis (pelo que pesquisei rapidamente no Google) pela cor vívida nos alimentos e bebidas; `magnesium` é um metal importante para o nosso corpo e é presente no vinho; e o `alcohol` é a porcentagem de álcool na bebida. Aqui, vemos que um vinho com um alto número neste componente seria provavelmente um vinho de cor vívida (pela alta influência dos `flavanoids` e por ter sinal positivo), com baixo teor alcoólico e níveis de magnésio (visto pelo sinal negativo, mas alta representatividade destes valores). Logo, quem sabe não poderíamos renomear este componente **7** para **cor_vivida_baixo_alcool_e_manganes**? Sim, não é um nome nada inspirado - mas creio que conseguiu captar a ideia.\n",
    "\n",
    "Finalmente, caso tenha interesse em ver o resultado final do PCA: observe como ficaria abaixo a tabela gerada com **4** componentes escolhidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pd.DataFrame(pca.fit_transform(df_wine_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além disso, vamos mostrar de forma gráfica como o PCA pode ajudar (e outras técnicas de seleção de atributos também). O `pairplot` é uma visualização a qual compara todos os atributos contra todos os atributos. Como nosso cérebro visualiza até 3 dimensões, para vermos mais do que isso precisamos recorrer para técnicas como esta. Veja que cada bolinha representa um produtor diferente. Note, ainda, que para algumas colunas conseguimos ver que há uma *certa* divisão entre os diferentes produtores, mas em outras fica difícil de fazermos esta separação.\n",
    "\n",
    "Ora, se até para nós humanos é difícil de nos acharmos aqui com a _consciência_ de que estamos analisando vinhos, imagine então como seria para um algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_plot = df_wine.copy() # criando uma cópia do df_wine para fins de visualização\n",
    "df_wine_plot['class'] = target_wine # colocando os produtores dentro desta cópia para podermos separá-los no gráfico\n",
    "sns.pairplot(df_wine_plot, hue='class') # criando o pairplot e dividindo por cor de produtor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora olhe para o mesmo `pairplot`, agora com o PCA aplicado. Tire as suas próprias conclusões. 🤐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_resultado = pd.DataFrame(pca.fit_transform(df_wine_scaled))\n",
    "df_pca_resultado['class'] = target_wine\n",
    "sns.pairplot(df_pca_resultado, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupamento\n",
    "### KMeans e MiniBatchKMeans\n",
    "\n",
    "O KMeans é a técnica mais conhecida de agrupamento (lembrando que agora estamos falando de agrupar **instâncias**, e não **atributos**). Neste caso e de forma similar ao que já fizemos anteriormente, aplicaremos o KMeans para o `df_wine`. Note que ele agrupa todos os dados disponíveis para a sua análise: logo, não é legal colocarmos uma classe/label no meio desta análise.\n",
    "\n",
    "Além disso, o `MiniBatchKMeans` é efetivamente o `KMeans` para grandes bases de dados. Logo, nos concentraremos somente no `KMeans`. Note que ele possui um parâmetro chamado `random_state`: ele é o que chamamos também de _seed_: um código aleatório que define um ponto de partida aleatório. Aqui, deixamos um valor fixo para que toda execução tenha o mesmo resultado, não importando de qual computador façamos os testes.\n",
    "\n",
    "Finalmente, para fins de visualização (lembra que o `sns.pairplot` com todas as colunas ficou gigante, certo?) estaremos já aplicando o `SelectKBest` para reduzir a quantidade de colunas sendo analisadas e colocadas em gráfico. Note também que estamos usando o `df_wine_scaled` como base da análise ao invés do `df_wine`: isto se dá porque precisamos que eles tenham uma mesma escala numérica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectKBest(k=5)\n",
    "df_wine_kbest = pd.DataFrame(sel.fit_transform(df_wine_scaled, target_wine), columns=df_wine.columns[sel.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    display(f'Resultados do KMeans com k={i}')\n",
    "    df_wine_kmeans = df_wine_kbest.copy()\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(df_wine_kmeans)\n",
    "    df_wine_kmeans['cluster'] = kmeans.predict(df_wine_kmeans)\n",
    "\n",
    "    sns.pairplot(df_wine_kmeans, hue='cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "O DBSCAN busca primeiramente encontrar exemplos que estão bem juntos/unidos/colados e, a partir daí, começa a encontrar os seus _vizinhos_. Ao contrário do KMeans, não requer um número de grupos/clusters. Todos os elementos que segundo ele não seriam membros de um cluster em específico ficariam dentro do cluster **-1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_dbscan = df_wine_kbest.copy()\n",
    "df_wine_dbscan['cluster'] = DBSCAN().fit_predict(df_wine_dbscan)\n",
    "\n",
    "sns.pairplot(df_wine_dbscan, hue='cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTICS\n",
    "\n",
    "O OPTICS é similar ao DBSCAN, mas pode levar a um agrupamento diferente uma vez que gera um cálculo que pode acabar por penalizar alguns elementos que não possuem tantos exemplos similares. Todos os elementos que segundo ele não seriam membros de um cluster em específico ficariam dentro do cluster **-1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_optics = df_wine_kbest.copy()\n",
    "df_wine_optics['cluster'] = OPTICS().fit_predict(df_wine_optics)\n",
    "\n",
    "sns.pairplot(df_wine_optics, hue='cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
